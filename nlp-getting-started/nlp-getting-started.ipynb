{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API is already configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# checking if kaggle API is configured\n",
    "\n",
    "if [ ! -d ~/.kaggle/ ]; then\n",
    "\n",
    "    echo \"Kaggle credentials are not configured\"\n",
    "\n",
    "else\n",
    "\n",
    "    echo \"Kaggle API is already configured\"\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already dataset downloaded.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# checking if data is downloaded\n",
    "\n",
    "if [ ! -d data/ ]; then\n",
    "\n",
    "    echo \"Downloading dataset...\"\n",
    "    kaggle competitions download nlp-getting-started\n",
    "\n",
    "    echo \"Unzipping datasets\"\n",
    "    unzip -qq nlp-getting-started.zip -d data/\n",
    "  \n",
    "    rm -rf nlp-getting-started.zip\n",
    "\n",
    "else\n",
    "\n",
    "    echo \"Dataset already dataset downloaded.\"\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "import tensorflow as tf\n",
    "\n",
    "!pip -q install transformers --user\n",
    "\n",
    "import re\n",
    "import transformers\n",
    "\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "MAX_SEQUENCE_LENGTH = 160\n",
    "\n",
    "LEARNING_RATE=2e-5\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=32\n",
    "DROPOUT=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'data/'\n",
    "\n",
    "train = pd.read_csv(ROOT_DIR + 'train.csv')\n",
    "test = pd.read_csv(ROOT_DIR + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 ms, sys: 0 ns, total: 140 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cleaning the data a bit\n",
    "\n",
    "df = pd.concat([train,test], sort=False)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_urls(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_emoji(x))\n",
    "df['text']=df['text'].apply(lambda x : re.sub(r'[^a-zA-Z#]+', ' ', x))\n",
    "\n",
    "train_cleaned = df[:train.shape[0]]\n",
    "test_cleaned = df[train.shape[0]:]\n",
    "\n",
    "test_cleaned = test_cleaned.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "train_input = bert_encode(train_cleaned.text.values, tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n",
    "test_input = bert_encode(test_cleaned.text.values, tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_labels = train_cleaned.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, is_vanilla=True):\n",
    "\n",
    "    token_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype=tf.int32, name='input_word_ids')\n",
    "    mask_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
    "    seg_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
    "\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(MODEL_TYPE)\n",
    "    seq_output, _ = bert_model([token_inputs, mask_inputs, seg_inputs])\n",
    "\n",
    "    if is_vanilla:\n",
    "        # just feeding into the final dense layer:\n",
    "        X = seq_output[:, 0, :]\n",
    "    else:\n",
    "        # pool -> dense -> dropout -> final dense\n",
    "        X = tf.keras.layers.GlobalAveragePooling1D()(seq_output)\n",
    "        X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(DROPOUT)(X)\n",
    "        \n",
    "    output_= tf.keras.layers.Dense(1, activation='sigmoid', name='output')(X)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Model([token_inputs, mask_inputs, seg_inputs],output_)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    #tf.keras.utils.plot_model(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===>Rate: 2e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.4541 - accuracy: 0.8034 - val_loss: 0.3902 - val_accuracy: 0.8286\n",
      "\n",
      "===>Rate: 2e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4361 - accuracy: 0.8123 - val_loss: 0.3953 - val_accuracy: 0.8253\n",
      "\n",
      "===>Rate: 2e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 155s 26ms/sample - loss: 0.4410 - accuracy: 0.8069 - val_loss: 0.3819 - val_accuracy: 0.8398\n",
      "\n",
      "===>Rate: 2e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4300 - accuracy: 0.8117 - val_loss: 0.3784 - val_accuracy: 0.8273\n",
      "\n",
      "===>Rate: 3e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.4410 - accuracy: 0.8153 - val_loss: 0.3772 - val_accuracy: 0.8332\n",
      "\n",
      "===>Rate: 3e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4348 - accuracy: 0.8122 - val_loss: 0.3704 - val_accuracy: 0.8411\n",
      "\n",
      "===>Rate: 3e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4418 - accuracy: 0.8028 - val_loss: 0.4031 - val_accuracy: 0.8299\n",
      "\n",
      "===>Rate: 3e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 157s 26ms/sample - loss: 0.4263 - accuracy: 0.8133 - val_loss: 0.3897 - val_accuracy: 0.8188\n",
      "\n",
      "===>Rate: 5e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 167s 27ms/sample - loss: 0.4328 - accuracy: 0.8123 - val_loss: 0.3864 - val_accuracy: 0.8194\n",
      "\n",
      "===>Rate: 5e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4450 - accuracy: 0.8072 - val_loss: 0.3992 - val_accuracy: 0.8332\n",
      "\n",
      "===>Rate: 5e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4418 - accuracy: 0.8069 - val_loss: 0.3657 - val_accuracy: 0.8418\n",
      "\n",
      "===>Rate: 5e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 157s 26ms/sample - loss: 0.4331 - accuracy: 0.8128 - val_loss: 0.3777 - val_accuracy: 0.8391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# recommended hyperparameters from \n",
    "learning_rate=[2e-5, 3e-5, 5e-5]\n",
    "epochs=[1]\n",
    "batch_size=[8, 16]\n",
    "dropout=[0.1]\n",
    "\n",
    "# trying two different architectures, vanilla BERT or + pool -> dense -> dropout \n",
    "is_vanilla=True\n",
    "\n",
    "# could use GridSearchCV and KerasClassifier but cannot handle multiple inputs\n",
    "for rate in learning_rate:\n",
    "    for batch in batch_size:\n",
    "        for is_vanilla in [True, False]:\n",
    "            print(\"\\n===>Rate: {}, batch size: {}, is_vanilla: {}\".format(rate,batch, is_vanilla))\n",
    "            model=create_model(rate, is_vanilla=is_vanilla)\n",
    "            history = model.fit(train_input,\n",
    "                train_labels,\n",
    "                validation_split=0.2,\n",
    "                epochs=1,\n",
    "                batch_size = batch,\n",
    "                verbose=1)\n",
    "            \n",
    "            submission = pd.read_csv(ROOT_DIR + 'sample_submission.csv')\n",
    "            yhat = model.predict(test_input)\n",
    "            submission['target'] = yhat.round().astype(int)\n",
    "            submission.to_csv('submission_bert_{}_{}_{}.csv'.format(rate, batch, is_vanilla), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 140s 23ms/sample - loss: 0.4445 - accuracy: 0.8000 - val_loss: 0.3772 - val_accuracy: 0.8332\n",
      "Epoch 2/2\n",
      "6090/6090 [==============================] - 123s 20ms/sample - loss: 0.3228 - accuracy: 0.8696 - val_loss: 0.4119 - val_accuracy: 0.8286\n",
      "CPU times: user 1min 32s, sys: 27.9 s, total: 2min\n",
      "Wall time: 4min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92e45c9400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = create_model(LEARNING_RATE, is_vanilla=False)\n",
    "model.fit(train_input,\n",
    "          train_labels,\n",
    "          validation_split=0.2,\n",
    "          epochs=EPOCHS,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(ROOT_DIR + 'sample_submission.csv')\n",
    "yhat = model.predict(test_input)\n",
    "submission['target'] = yhat.round().astype(int)\n",
    "submission.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c nlp-getting-started -f submission_bert.csv -m \"testing API submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
