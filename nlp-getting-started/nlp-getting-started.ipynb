{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API is already configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# checking if kaggle API is configured\n",
    "\n",
    "if [ ! -d ~/.kaggle/ ]; then\n",
    "\n",
    "    echo \"Kaggle credentials are not configured\"\n",
    "\n",
    "else\n",
    "\n",
    "    echo \"Kaggle API is already configured\"\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already dataset downloaded.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# checking if data is downloaded\n",
    "\n",
    "if [ ! -d data/ ]; then\n",
    "\n",
    "  echo \"Downloading dataset...\"\n",
    "  kaggle competitions download nlp-getting-started\n",
    "\n",
    "  echo \"Unzipping datasets\"\n",
    "  unzip -qq nlp-getting-started.zip -d data/\n",
    "  \n",
    "  rm -rf nlp-getting-started.zip\n",
    "\n",
    "else\n",
    "\n",
    "  echo \"Dataset already dataset downloaded.\"\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl\n",
      "Processing /home/jupyter/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422/sacremoses-0.0.38-cp35-none-any.whl\n",
      "Processing /home/jupyter/.cache/pip/wheels/1c/78/87/21be0303007ee5d1483df56703c9c7e5a44873e8f0c51d65f8/regex-2020.1.8-cp35-cp35m-linux_x86_64.whl\n",
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/d5/57/e9675a5a8d0ee586594ff19cb9a601334fbf24fa2fb29052d2a900ee5d23/boto3-1.11.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/site-packages (from transformers) (1.15.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages (from transformers) (4.40.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.5/dist-packages (from transformers) (2.22.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached https://files.pythonhosted.org/packages/4b/7b/97cdb2425ae93cdb3231ba38edb759bb311751c16bebda242508f9a79682/sentencepiece-0.1.85-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (1.13.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Collecting botocore<1.15.0,>=1.14.9\n",
      "  Using cached https://files.pythonhosted.org/packages/64/4c/b0b0d3b6f84a05f9135051b56d3eb8708012a289c4b82ee21c8c766f47b5/botocore-1.14.9-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c7/48/a8252b6b3cd31774eab312b19d58a6ac55f296240c206617dcd38cd93bf8/s3transfer-0.3.2-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.5/dist-packages (from numpy->transformers) (2019.0)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python3.5/dist-packages (from numpy->transformers) (2020.0.133)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.5/dist-packages (from numpy->transformers) (2019.0)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.5/dist-packages (from numpy->transformers) (1.0.6)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python3.5/dist-packages (from numpy->transformers) (1.0.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests->transformers) (2.8)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.5/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.5/dist-packages (from mkl->numpy->transformers) (2020.0.133)\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python3.5/dist-packages (from tbb4py->numpy->transformers) (2019.0)\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python3.5/dist-packages (from mkl-fft->numpy->transformers) (1.15.1)\n",
      "Installing collected packages: regex, sacremoses, jmespath, docutils, botocore, s3transfer, boto3, sentencepiece, transformers\n",
      "Successfully installed boto3-1.11.9 botocore-1.14.9 docutils-0.15.2 jmespath-0.9.4 regex-2020.1.8 s3transfer-0.3.2 sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "!pip install transformers --user\n",
    "\n",
    "import re\n",
    "import transformers\n",
    "\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "MAX_SEQUENCE_LENGTH = 160\n",
    "\n",
    "LEARNING_RATE=2e-5\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=32\n",
    "DROPOUT=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'data/'\n",
    "\n",
    "train = pd.read_csv(ROOT_DIR + 'train.csv')\n",
    "test = pd.read_csv(ROOT_DIR + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 0 ns, total: 144 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cleaning the data a bit\n",
    "\n",
    "df = pd.concat([train,test], sort=False)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_urls(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_emoji(x))\n",
    "df['text']=df['text'].apply(lambda x : re.sub(r'[^a-zA-Z#]+', ' ', x))\n",
    "\n",
    "train_cleaned = df[:train.shape[0]]\n",
    "test_cleaned = df[train.shape[0]:]\n",
    "\n",
    "test_cleaned = test_cleaned.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "train_input = bert_encode(train_cleaned.text.values, tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n",
    "test_input = bert_encode(test_cleaned.text.values, tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_labels = train_cleaned.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, is_vanilla=True):\n",
    "\n",
    "    token_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype=tf.int32, name='input_word_ids')\n",
    "    mask_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
    "    seg_inputs = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
    "\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(MODEL_TYPE)\n",
    "    seq_output, _ = bert_model([token_inputs, mask_inputs, seg_inputs])\n",
    "\n",
    "    if is_vanilla:\n",
    "        # just feeding into the final dense layer:\n",
    "        X = seq_output[:, 0, :]\n",
    "    else:\n",
    "        # pool -> dense -> dropout -> final dense\n",
    "        X = tf.keras.layers.GlobalAveragePooling1D()(seq_output)\n",
    "        X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(DROPOUT)(X)\n",
    "        \n",
    "    output_= tf.keras.layers.Dense(1, activation='sigmoid', name='output')(X)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Model([token_inputs, mask_inputs, seg_inputs],output_)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    #tf.keras.utils.plot_model(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===>Rate: 2e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 167s 27ms/sample - loss: 0.4347 - accuracy: 0.8117 - val_loss: 0.3963 - val_accuracy: 0.8240\n",
      "\n",
      "===>Rate: 2e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4320 - accuracy: 0.8177 - val_loss: 0.3961 - val_accuracy: 0.8214\n",
      "\n",
      "===>Rate: 2e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4390 - accuracy: 0.8041 - val_loss: 0.4057 - val_accuracy: 0.8306\n",
      "\n",
      "===>Rate: 2e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4421 - accuracy: 0.8094 - val_loss: 0.3841 - val_accuracy: 0.8365\n",
      "\n",
      "===>Rate: 3e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4369 - accuracy: 0.8117 - val_loss: 0.3813 - val_accuracy: 0.8273\n",
      "\n",
      "===>Rate: 3e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4390 - accuracy: 0.8085 - val_loss: 0.3668 - val_accuracy: 0.8319\n",
      "\n",
      "===>Rate: 3e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 155s 26ms/sample - loss: 0.4411 - accuracy: 0.8066 - val_loss: 0.4054 - val_accuracy: 0.8188\n",
      "\n",
      "===>Rate: 3e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4359 - accuracy: 0.8090 - val_loss: 0.3663 - val_accuracy: 0.8306\n",
      "\n",
      "===>Rate: 5e-05, batch size: 8, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4447 - accuracy: 0.8030 - val_loss: 0.3748 - val_accuracy: 0.8372\n",
      "\n",
      "===>Rate: 5e-05, batch size: 8, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.4396 - accuracy: 0.8120 - val_loss: 0.4003 - val_accuracy: 0.8240\n",
      "\n",
      "===>Rate: 5e-05, batch size: 16, is_vanilla: True\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 157s 26ms/sample - loss: 0.4363 - accuracy: 0.8136 - val_loss: 0.3879 - val_accuracy: 0.8359\n",
      "\n",
      "===>Rate: 5e-05, batch size: 16, is_vanilla: False\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 156s 26ms/sample - loss: 0.4336 - accuracy: 0.8108 - val_loss: 0.4029 - val_accuracy: 0.8240\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# recommended hyperparameters from \n",
    "learning_rate=[2e-5, 3e-5, 5e-5]\n",
    "epochs=[1]\n",
    "batch_size=[8, 16]\n",
    "dropout=[0.1]\n",
    "\n",
    "# trying two different architectures, vanilla BERT or + pool -> dense -> dropout \n",
    "is_vanilla=True\n",
    "\n",
    "# could use GridSearchCV and KerasClassifier but cannot handle \n",
    "for rate in learning_rate:\n",
    "    for batch in batch_size:\n",
    "        for is_vanilla in [True, False]:\n",
    "            print(\"\\n===>Rate: {}, batch size: {}, is_vanilla: {}\".format(rate,batch, is_vanilla))\n",
    "            model=create_model(rate, is_vanilla=is_vanilla)\n",
    "            history = model.fit(train_input,\n",
    "                train_labels,\n",
    "                validation_split=0.2,\n",
    "                epochs=1,\n",
    "                batch_size = batch,\n",
    "                verbose=1)\n",
    "            \n",
    "            submission = pd.read_csv(ROOT_DIR + 'sample_submission.csv')\n",
    "            yhat = model.predict(test_input)\n",
    "            submission['target'] = yhat.round().astype(int)\n",
    "            submission.to_csv('submission_bert_{}_{}_{}.csv'.format(rate, batch, is_vanilla), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "6090/6090 [==============================] - 140s 23ms/sample - loss: 0.4378 - accuracy: 0.8138 - val_loss: 0.4061 - val_accuracy: 0.8162\n",
      "Epoch 2/2\n",
      "6090/6090 [==============================] - 123s 20ms/sample - loss: 0.3239 - accuracy: 0.8706 - val_loss: 0.4257 - val_accuracy: 0.8188\n",
      "CPU times: user 1min 32s, sys: 27.8 s, total: 2min\n",
      "Wall time: 4min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f58c1869278>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = create_model(LEARNING_RATE, is_vanilla=False)\n",
    "model.fit(train_input,\n",
    "          train_labels,\n",
    "          validation_split=0.2,\n",
    "          epochs=EPOCHS,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(ROOT_DIR + 'sample_submission.csv')\n",
    "yhat = model.predict(test_input)\n",
    "submission['target'] = yhat.round().astype(int)\n",
    "submission.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 22.2k/22.2k [00:03<00:00, 6.21kB/s]\n",
      "Successfully submitted to Real or Not? NLP with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission_bert.csv -m \"testing API submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
